{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath('.')\n",
    "\n",
    "QUERY_URL = 'http://www.iwencai.com/data-robot/search?qs=pc_~soniu~stock~resultpage~datarobot~zdjgy&query=%s&isChatBot=0&direct_mode=[\"iwencai\"]'\n",
    "QUERY_COLUMN = [\n",
    "    '股票代码', '股票简称', '收盘价不复权', '收盘价前复权', '收盘价后复权', '总市值', '流通市值', '市净率', '市盈率',\n",
    "    '市销率', '市现率', '成交额', '成交量', '股本', '每股净资产', '净资产收益率', '总资产', '净资产', '负债',\n",
    "    '货币资金', '营业收入', '净利润', '未分配利润', '营业收入增长率', '资产负债率', '毛利率', '净利率', '发行价',\n",
    "    '首日开盘价', '申万行业', '证监会行业', '同花顺行业', '上市时间'\n",
    "]\n",
    "\n",
    "STATISTIC_COLUMN = [\n",
    "    '收盘价不复权', '收盘价前复权', '收盘价后复权', '总市值', '流通市值', '市净率', '市盈率', '市销率', '市现率',\n",
    "    '成交额', '成交量', '股本', '每股净资产', '净资产收益率', '总资产', '净资产', '负债', '货币资金', '营业收入',\n",
    "    '净利润', '未分配利润', '营业收入增长率', '资产负债率', '毛利率', '净利率', '发行价', '首日开盘价'\n",
    "]\n",
    "\n",
    "CLASSICFICATION = [\n",
    "    '采掘', '传媒', '电气设备', '电子', '房地产', '纺织服装', '非银金融', '钢铁', '公用事业', '国防军工',\n",
    "    '化工', '机械设备', '计算机', '家用电器', '建筑材料', '建筑装饰', '交通运输', '农林牧渔', '汽车', '轻工制造',\n",
    "    '商业贸易', '食品饮料', '通信', '休闲服务', '医药生物', '银行', '有色金属', '综合'\n",
    "]\n",
    "\n",
    "DATA_DIR = ROOT_DIR + '\\\\data'\n",
    "BASIC_DATA_DIR = DATA_DIR + '\\\\basic'\n",
    "STATISTIC_DATA_DIR = DATA_DIR + '\\\\statistic'\n",
    "CLASSIFIED_DATA_DIR = DATA_DIR + '\\\\classified'\n",
    "STOCK_DATA_DIR = DATA_DIR + '\\\\stock'\n",
    "TRADE_DATE_FILE = DATA_DIR + '\\\\date_week.txt'\n",
    "ALL_STOCK_FILE = DATA_DIR + '\\\\stock.data'\n",
    "STATISTIC_FILE = DATA_DIR + '\\\\statistic.data'\n",
    "CLASSIFIED_FILE = DATA_DIR + '\\\\classified.data'\n",
    "MARKET = [{\n",
    "    'name': '全部市场',\n",
    "    'column': '股票代码',\n",
    "    'filter': ''\n",
    "}, {\n",
    "    'name': '上海市场',\n",
    "    'column': '股票代码',\n",
    "    'filter': r'^6'\n",
    "}, {\n",
    "    'name': '深圳市场',\n",
    "    'column': '股票代码',\n",
    "    'filter': r'^00[0-1]'\n",
    "}, {\n",
    "    'name': '中小板市场',\n",
    "    'column': '股票代码',\n",
    "    'filter': r'^002'\n",
    "}, {\n",
    "    'name': '创业板市场',\n",
    "    'column': '股票代码',\n",
    "    'filter': r'^3'\n",
    "}, {\n",
    "    'name': '2014.01.01老股',\n",
    "    'column': '上市时间',\n",
    "    'filter': r'[0-1]|^200[0-9]|^201[0-3]'\n",
    "}, {\n",
    "    'name': '2014.01.01新股',\n",
    "    'column': '上市时间',\n",
    "    'filter': r'^[3-9]|^2[1-9]|^20[2-9]|^201[4-9]'\n",
    "}]\n",
    "\n",
    "METHOD = [{\n",
    "    'name': '求个数',\n",
    "    'method': 'count'\n",
    "}, {\n",
    "    'name': '标准差',\n",
    "    'method': 'std'\n",
    "}, {\n",
    "    'name': '最小值',\n",
    "    'method': 'min'\n",
    "}, {\n",
    "    'name': '25%分位',\n",
    "    'method': '25%'\n",
    "}, {\n",
    "    'name': '50%分位',\n",
    "    'method': '50%'\n",
    "}, {\n",
    "    'name': '75%分位',\n",
    "    'method': '75%'\n",
    "}, {\n",
    "    'name': '最大值',\n",
    "    'method': 'max'\n",
    "}, {\n",
    "    'name': '算术平均数',\n",
    "    'method': 'mean'\n",
    "}, {\n",
    "    'name': '求和数',\n",
    "    'method': 'sum'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 同花顺问财iwencai数据机器人数据导出\n",
    "\n",
    "## 数据访问方式\n",
    "\n",
    "http://www.iwencai.com/data-robot/search?qs=pc_~soniu~stock~resultpage~datarobot~zdjgy&query=%s&isChatBot=0&direct_mode=[\"iwencai\"]\n",
    "\n",
    "* 这个查询接口曾经出现过一段时间，现在已经在问财网站上找不到公开的调用\n",
    "\n",
    "关键在于查询问句query，比如 “今年创新低的股票”，“2011.1.1 股价，PB”...\n",
    "\n",
    "返回的是json格式\n",
    "\n",
    "\n",
    "## 主要思路\n",
    "\n",
    "### 用途\n",
    "\n",
    "用于导出个股历史数据，形成本地的基础数据库，以便于保存 统计 查询 等功能\n",
    "\n",
    "导出频率以一周为一个数据快照\n",
    "\n",
    "### 功能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_method_by_name(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method_by_name(name):\n",
    "    for mt in METHOD:\n",
    "        if name == mt['name']:\n",
    "            return mt['method']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_Snapshot(date,  query)\n",
    "Parameters：\n",
    "date   查询快照日期\n",
    "query  查询快照问句\n",
    "\n",
    "Return:\n",
    "返回查询结果dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Snapshot(date, query):\n",
    "    global response\n",
    "\n",
    "    qstr = pd.Series(query)\n",
    "    request = QUERY_URL % urllib.parse.quote(qstr.str.cat(sep='[%s],' % date))\n",
    "    try:\n",
    "        response = urllib.request.urlopen(request)\n",
    "    except urllib.error.URLError as e:\n",
    "        print(e.reason)\n",
    "    j_data = json.loads(response.read())\n",
    "    ss = pd.DataFrame(j_data['data']['result'])\n",
    "    ss.columns = QUERY_COLUMN\n",
    "    ss = ss.dropna(subset=['上市时间'], axis=0, how='any')\n",
    "    ss = ss[ss['上市时间'] <= date]\n",
    "    ss = ss.reset_index(drop=True)\n",
    "    ss = ss.fillna(0)\n",
    "    for col in STATISTIC_COLUMN:\n",
    "        ss[col] = ss[col].astype(float)\n",
    "    ss = optimize_dataframe(ss)\n",
    "\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_Snapshot('20180108', QUERY_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimize_dataframe(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dataframe(dataframe):\n",
    "    for col in dataframe.columns:\n",
    "        if (dataframe[col].dtype == 'object') and (col != 'date') and (col !=\n",
    "                                                                       ('date',\n",
    "                                                                        '')):\n",
    "            dataframe[col] = dataframe[col].astype('category')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save_Snapshot(snapshot, filename)\n",
    "用Pickle方式序列化snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Snapshot(snapshot, filename, overwrite=True):\n",
    "    if overwrite:\n",
    "        return optimize_dataframe(snapshot).to_pickle(filename)\n",
    "    else:\n",
    "        if os.path.exists(filename):\n",
    "            print('已存在该数据文件%s.' % filename)\n",
    "            return False\n",
    "        else:\n",
    "            return optimize_dataframe(snapshot).to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load_Snapshot(filename)\n",
    "读取snapshot\n",
    "\n",
    "Parameters:\n",
    "\n",
    "filename snapshot文件\n",
    "\n",
    "Return:\n",
    "\n",
    "返回dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_Snapshot(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print('不存在该数据文件%s.' % filename)\n",
    "        return False\n",
    "    else:\n",
    "        #return optimize_dataframe(pd.read_pickle(filename))\n",
    "        return pd.read_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_to_str(date):\n",
    "    d = date.split('/', 3)\n",
    "    return '%04d%02d%02d' % (int(d[0]), int(d[1]), int(d[2]))\n",
    "\n",
    "\n",
    "def get_TradeDate():\n",
    "    filename = TRADE_DATE_FILE\n",
    "    if os.path.exists(filename):\n",
    "        wd = pd.read_table(filename, encoding='utf-8', header=None)\n",
    "        return wd\n",
    "    else:\n",
    "        print('不存在该日期文件%s.' % filename)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_local_Snapshot(start_end, query)\n",
    "查询需要的问句，并保存到本地，形成本地数据库。一个文件代表一天的数据快照。\n",
    "\n",
    "Parameters：\n",
    "\n",
    "start_end    查询开始结束时间，是一个日期list\n",
    "\n",
    "query        查询问句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_local_Snapshot(start_end, query):\n",
    "    # time.sleep(0.1)\n",
    "    print('正在重建本地基础数据库(交易日快照)，耗时比较长，请耐心等候！', end='')\n",
    "\n",
    "    total = len(start_end)\n",
    "    i = 0\n",
    "    start = datetime.datetime.now()\n",
    "    # 下载基础数据库，一个date，一个数据文件\n",
    "    for date in start_end:\n",
    "        # 日快照\n",
    "        filename = '%s/%s.data' % (BASIC_DATA_DIR, date_to_str(date))\n",
    "        if not os.path.exists(filename):\n",
    "            ss = get_Snapshot(date_to_str(date), query)\n",
    "            save_Snapshot(ss, filename)\n",
    "        else:\n",
    "            ss = load_Snapshot(filename)\n",
    "\n",
    "        # 日统计快照\n",
    "        stat_list = []\n",
    "        filename = '%s/%s.data' % (STATISTIC_DATA_DIR, date_to_str(date))\n",
    "        if not os.path.exists(filename):\n",
    "            for mt in MARKET:\n",
    "                dt = ss[ss[mt['column']].str.contains(mt['filter'])]\n",
    "                desc = dt.describe()\n",
    "                desc.loc['sum'] = dt.sum()\n",
    "                desc['date'] = date_to_str(date)\n",
    "                desc['market'] = mt['name']\n",
    "                stat_list.append(desc)\n",
    "            # NaN 用 0 填充\n",
    "            statistic = pd.concat(stat_list)\n",
    "            statistic = statistic.fillna(0)\n",
    "            save_Snapshot(statistic, filename)\n",
    "\n",
    "        # 日行业统计快照\n",
    "        filename = '%s/%s.data' % (\n",
    "            CLASSIFIED_DATA_DIR, date_to_str(date))\n",
    "        ss['行业'] = ss['申万行业'].apply(lambda x: x.split('-')[0])        \n",
    "        if not os.path.exists(filename):\n",
    "            #ss['行业'] = ss['申万行业'].apply(lambda x: x.split('-')[0])\n",
    "            dt = ss.groupby(['行业'])\n",
    "            desc = dt.agg([\n",
    "                'count', 'std', 'min', ('25%',\n",
    "                                        lambda x: pd.Series.quantile(x, .25)),\n",
    "                ('50%', lambda x: pd.Series.quantile(x, .5)),\n",
    "                ('75%', lambda x: pd.Series.quantile(x, .75)), 'max', 'mean', 'sum'\n",
    "            ])\n",
    "            desc['date']=date_to_str(date)\n",
    "            # NaN 用 0 填充\n",
    "            desc = desc.fillna(0)\n",
    "            desc = desc.reset_index()\n",
    "            save_Snapshot(desc, filename)\n",
    "\n",
    "        print(\n",
    "            '[本地基础数据库(交易日快照)] 数据处理中(%d/%d)，预计需时间:%s' %\n",
    "            (i + 1, total,\n",
    "             (datetime.datetime.now() - start) / (i + 1) * (total - i - 1)),\n",
    "            end='')\n",
    "        i += 1\n",
    "    print('重建本地基础数据库(交易日快照)完毕！', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#build_local_Snapshot(get_TradeDate()[0], QUERY_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_stock_Snapshot()\n",
    "将下载的每天数据快照，全部整合成一个数据快照，方便做一些中间的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stock_Snapshot():\n",
    "    print('正在重建整合数据，耗时比较长，请耐心等候！', end='')\n",
    "    stock_list = []\n",
    "    start = datetime.datetime.now()\n",
    "    for _, _, filenames in os.walk(BASIC_DATA_DIR):\n",
    "        i = 0\n",
    "        total = len(filenames)\n",
    "        start = datetime.datetime.now()\n",
    "        # 根据下载的基础数据库，全部整合到一个数据文件\n",
    "        for filename in filenames:\n",
    "            fname = os.path.splitext(filename)\n",
    "            date = fname[0]\n",
    "            ss = load_Snapshot('%s/%s' % (BASIC_DATA_DIR, filename))\n",
    "            ss['date'] = date\n",
    "            stock_list.append(ss)\n",
    "            print(\n",
    "                '[整合数据] 数据处理中(%d/%d)，预计需时间:%s' %\n",
    "                (i + 1, total, (datetime.datetime.now() - start) / (i + 1) *\n",
    "                 (total - i - 1)),\n",
    "                end='')\n",
    "            i += 1\n",
    "    # NaN 用 0 填充\n",
    "    stock = pd.concat(stock_list)\n",
    "    stock = stock.fillna(0)\n",
    "    save_Snapshot(stock, ALL_STOCK_FILE)\n",
    "    print('重建本地整合数据完毕！', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_stock_Snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split_stock_Snapshot()\n",
    "把整合全部的数据快照，按照股票代码切割出来。形成一个文件是一个股票历史数据快照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_stock_Snapshot():\n",
    "    print('正在重建本地基础数据库(个股快照)，耗时比较长，请耐心等候！', end='')\n",
    "    if not os.path.exists(ALL_STOCK_FILE):\n",
    "        build_stock_Snapshot()\n",
    "    else:\n",
    "        stock = load_Snapshot(ALL_STOCK_FILE)\n",
    "        stk = stock.drop_duplicates(subset=['股票代码'], keep='first')\n",
    "        i = 0\n",
    "        total = len(stk)\n",
    "        start = datetime.datetime.now()\n",
    "        # 将切割成一个股票，一个历史文件\n",
    "        for code in stk['股票代码']:\n",
    "            s = stock[stock['股票代码'].str.contains(code)]\n",
    "            s = s.reset_index(drop=True)\n",
    "\n",
    "            filename = '%s/%s.data' % (STOCK_DATA_DIR, code.split('.', 2)[0])\n",
    "            save_Snapshot(s, filename)\n",
    "            print(\n",
    "                '[本地基础数据库(个股快照)] 数据处理中(%d/%d)，预计需时间:%s' %\n",
    "                (i + 1, total, (datetime.datetime.now() - start) / (i + 1) *\n",
    "                 (total - i - 1)),\n",
    "                end='')\n",
    "            i += 1\n",
    "    print('重建本地基础数据库(个股快照)完毕！', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_statistic_Snapshot()\n",
    "对每天的数据快照做一个统计计算，并合并成一个统计数据快照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_statistic_Snapshot():\n",
    "    print('正在重建个股统计数据，耗时比较长，请耐心等候！', end='')\n",
    "    stock_list = []\n",
    "    start = datetime.datetime.now()\n",
    "    for _, _, filenames in os.walk(STATISTIC_DATA_DIR):\n",
    "        i = 0\n",
    "        total = len(filenames)\n",
    "        start = datetime.datetime.now()\n",
    "        # 根据下载的基础数据库，全部整合到一个数据文件\n",
    "        for filename in filenames:\n",
    "            ss = load_Snapshot('%s/%s' % (STATISTIC_DATA_DIR, filename))\n",
    "            stock_list.append(ss)\n",
    "            print(\n",
    "                '[个股统计] 数据处理中(%d/%d)，预计需时间:%s' %\n",
    "                (i + 1, total, (datetime.datetime.now() - start) / (i + 1) *\n",
    "                 (total - i - 1)),\n",
    "                end='')\n",
    "            i += 1\n",
    "    # NaN 用 0 填充\n",
    "    stock = pd.concat(stock_list)\n",
    "    #stock = stock.fillna(0)\n",
    "    save_Snapshot(stock, STATISTIC_FILE)    \n",
    "    print('重建个股统计数据完毕！', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_statistic_Snapshot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_classified_Snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def build_classified_Snapshot():\n",
    "    print('正在重建行业统计数据，耗时比较长，请耐心等候！', end='')\n",
    "    stock_list = []\n",
    "    start = datetime.datetime.now()\n",
    "    for _, _, filenames in os.walk(CLASSIFIED_DATA_DIR):\n",
    "        i = 0\n",
    "        total = len(filenames)\n",
    "        start = datetime.datetime.now()\n",
    "        # 根据下载的基础数据库，全部整合到一个数据文件\n",
    "        for filename in filenames:\n",
    "            ss = load_Snapshot('%s/%s' % (CLASSIFIED_DATA_DIR, filename))\n",
    "            stock_list.append(ss)\n",
    "            print(\n",
    "                '[重建行业] 数据处理中(%d/%d)，预计需时间:%s' %\n",
    "                (i + 1, total, (datetime.datetime.now() - start) / (i + 1) *\n",
    "                 (total - i - 1)),\n",
    "                end='')\n",
    "            i += 1\n",
    "    # NaN 用 0 填充\n",
    "    stock = pd.concat(stock_list)\n",
    "    stock = stock.fillna(0)\n",
    "    save_Snapshot(stock, CLASSIFIED_FILE)\n",
    "    print('重建行业统计数据完毕！', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_classified_Snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#build_classified_Snapshot()\n",
    "#build_statistic_Snapshot()\n",
    "#f = load_Snapshot('%s/%s.data' % (STATISTIC_DATA_DIR, '20180212'))\n",
    "#df = load_Snapshot(STATISTIC_FILE)\n",
    "#df = df[df['date']=='20130722']\n",
    "#df.to_excel('1.xlsx')\n",
    "#[max(df[c].apply(lambda x: len(str(x)))) for c in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_Snapshot(CLASSIFIED_FILE)\n",
    "#df.info(memory_usage='deep')\n",
    "#df[df['股票代码'].str.contains(r'^002')]\n",
    "#df['date'] = df['date'].apply(lambda x: date_to_str(x))\n",
    "#save_Snapshot(df, CLASSIFIED_FILE)    \n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "815px",
    "left": "0px",
    "right": "1708px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "654px",
    "left": "1537px",
    "right": "20px",
    "top": "283px",
    "width": "342px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
